{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Matisse Colombon s4119576"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gsarti/ik-nlp-tutorials/blob/main/notebooks/W4E_NonTextual_Information.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:14:11.059485949Z",
     "start_time": "2024-03-11T13:14:08.369023971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (3.7.2)\r\n",
      "Requirement already satisfied: transformers in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (4.16.2)\r\n",
      "Requirement already satisfied: sentencepiece in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (0.1.99)\r\n",
      "Requirement already satisfied: datasets in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (2.17.1)\r\n",
      "Requirement already satisfied: scikit-learn in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (1.4.0)\r\n",
      "Requirement already satisfied: pandas in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (2.1.4)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (8.2.2)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (0.9.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (6.4.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (4.66.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (2.5.3)\r\n",
      "Requirement already satisfied: jinja2 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (3.1.3)\r\n",
      "Requirement already satisfied: setuptools in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (59.5.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (23.2)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from spacy) (1.26.3)\r\n",
      "Requirement already satisfied: filelock in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from transformers) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from transformers) (0.20.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: sacremoses in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from transformers) (0.0.53)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from transformers) (0.13.3)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from datasets) (14.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from datasets) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from datasets) (0.3.7)\r\n",
      "Requirement already satisfied: xxhash in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from datasets) (0.70.15)\r\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from datasets) (2023.10.0)\r\n",
      "Requirement already satisfied: aiohttp in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from datasets) (3.9.1)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from pandas) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from pandas) (2023.4)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.9.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\r\n",
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "^C\r\n",
      "\r\n",
      "\u001B[31mAborted.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Run in Colab to install local packages\n",
    "!pip install spacy transformers sentencepiece datasets scikit-learn pandas\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Textual and Non-textual Features in NLP Models\n",
    "\n",
    "*Based on the Text Classification tutorial by [Debora Nozza](https://dnozza.github.io/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many real-world applications, text is just one of the multiple sources of information that can be used to predict desired quantities. In this exercise, you will reproduce a standard machine learning pipeline integrating text and non-textual information. Importantly, while in the previous tutorials and exercises we focused on the usage of advanced tooling such as the Transformers library, here we will start from the basics to establish some baseline results using the popular [Scikit-learn](https://scikit-learn.org/stable/index.html) library. \n",
    "\n",
    "**Exercise 1**, which is mandatory and will be part of your graded midterm portfolio, will include the following steps:\n",
    "\n",
    "1. Preprocess the text to extract lemmatized content words.\n",
    "\n",
    "2. Converting the text to a vector representation using simple count-based approaches.\n",
    "\n",
    "3. Convert categorical features into one-hot vectors.\n",
    "\n",
    "4. Fit a simple model to predict the desired target.\n",
    "\n",
    "5. Establish a simple baseline performance for the prediction task.\n",
    "\n",
    "6. Evaluate the model performance on a held-out set.\n",
    "\n",
    "7. Perform a feature selection and re-evaluate the model\n",
    "\n",
    "8. Obtain insights about salient words and features for the prediction task.\n",
    "\n",
    "Every operation to be completed is marked with a `TODO` comment in the code section. While these represent a small but nonetheless comprehensive set of steps to train models on textual and non-textual information, nowadays NLP practitioners operate mainly with pre-trained word embeddings for representing textual information. \n",
    "\n",
    "In **Exercise 2** (optional), you will be asked to replace our text representation with a modern transformer-based model and evaluate the difference with respect to previous results. As always, we recommend you to complete this optional exercise, especially if you plan to deal with non-textual data (e.g. quality estimation scores, translator's behavioral data) during your final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: A Simple Wine Scoring Pipeline\n",
    "\n",
    "In this exercise, we will use a filtered version of the [Winemag dataset](https://www.kaggle.com/zynicide/wine-reviews) containing a collection of wine reviews (`description`), accompanied by some metadata: `country` and `province` of provenance, `variety` of wine, `price` per bottle and the WineEnthusiast rating (`points`) describing the wine quality.\n",
    "\n",
    "*Your final goal is to build and evaluate a simple linear regression model that predicts the `points` assigned to a wine given its `description` and its other features.* This is commonly known as a **regression problem**, since you are trying to predict a continuous quantity, as opposed to a discrete one (e.g. a class label).\n",
    "\n",
    "Most importantly, the general procedure and methods you will use can be applied to any kind of data with the adequate preprocessing, and can be extended to other tasks such as binary and multiclass classification.\n",
    "\n",
    "You can have a look at the data, which has been conveniently packed into a Huggingface Dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-11T13:15:55.091665530Z",
     "start_time": "2024-03-11T13:15:53.457573445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'country', 'description', 'points', 'price', 'province', 'variety'],\n",
      "        num_rows: 70458\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'country', 'description', 'points', 'price', 'province', 'variety'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'country', 'description', 'points', 'price', 'province', 'variety'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": "    index country                                        description  points  \\\n0  129857      US  Dusty tannins make for a soft texture in this ...      90   \n1  112217      US  Sweet-tart Maraschino cherry and bitter brambl...      85   \n2  114216  France  A lightly orange-colored rosé that is made by ...      92   \n3   37808  France  A ripe wine that is almost off dry, this has a...      85   \n4   31157      US  Crisp and very floral, this is a beautiful sho...      92   \n\n   price    province                   variety  \n0   44.0  California                    Merlot  \n1   14.0    New York                Pinot Noir  \n2   90.0   Champagne           Champagne Blend  \n3   17.0    Bordeaux  Bordeaux-style Red Blend  \n4   20.0  California                Pinot Gris  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>country</th>\n      <th>description</th>\n      <th>points</th>\n      <th>price</th>\n      <th>province</th>\n      <th>variety</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>129857</td>\n      <td>US</td>\n      <td>Dusty tannins make for a soft texture in this ...</td>\n      <td>90</td>\n      <td>44.0</td>\n      <td>California</td>\n      <td>Merlot</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>112217</td>\n      <td>US</td>\n      <td>Sweet-tart Maraschino cherry and bitter brambl...</td>\n      <td>85</td>\n      <td>14.0</td>\n      <td>New York</td>\n      <td>Pinot Noir</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>114216</td>\n      <td>France</td>\n      <td>A lightly orange-colored rosé that is made by ...</td>\n      <td>92</td>\n      <td>90.0</td>\n      <td>Champagne</td>\n      <td>Champagne Blend</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>37808</td>\n      <td>France</td>\n      <td>A ripe wine that is almost off dry, this has a...</td>\n      <td>85</td>\n      <td>17.0</td>\n      <td>Bordeaux</td>\n      <td>Bordeaux-style Red Blend</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>31157</td>\n      <td>US</td>\n      <td>Crisp and very floral, this is a beautiful sho...</td>\n      <td>92</td>\n      <td>20.0</td>\n      <td>California</td>\n      <td>Pinot Gris</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"GroNLP/ik-nlp-22_winemag\")\n",
    "print(data)\n",
    "data[\"train\"].to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "Text is messy. The goal of preprocessing is to reduce the amount of noise (= unnecessary variation), while maintaining the signal. There is no one-size-fits-all solution, but a good approximation can be, for example, to preserve only content words and reduce the size of the vocabulary by means of a lemmatizer.\n",
    "\n",
    "You learned how to extract lemmas and POS tags using spaCy, and how to use `.map` to apply a function to a `Dataset`, so you should have all the tools to succeed in this. Fill in the missing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-11T13:15:59.405560020Z",
     "start_time": "2024-03-11T13:15:59.000033471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'test sentence here come one go'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Disable unused components\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Reduce text to lower-case lemmatized content words.'''\n",
    "    content_op = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN']\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_.lower() for token in doc if token.pos_ in content_op]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "clean_text('This is a test sentence. And here comes another one... Go me!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now apply this cleaning function to the `description` column of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:16:14.529711635Z",
     "start_time": "2024-03-11T13:16:00.840053333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db89bdfe6fb94cbdbb2dc24fe0d4a21d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split in data.keys():\n",
    "    # description column, mapping the output to a new clean_text column\n",
    "    # and removing the original description column.\n",
    "    data[split] = data[split].map(lambda x: {'clean_text': clean_text(x['description'])}, remove_columns=['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Text\n",
    "\n",
    "Now that you have a more compact representation of the text, the next step is converting it into a vector representation. For this purpose, you will use the [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class provided by Scikit-learn. This class converts a collection of text documents to a matrix of [TF-IDF scores](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) reflecting the importance of a word in a document, and in relation to the full corpus. We are going to set some parameters to ensure a limited size of the vocabulary, but you can experiment with other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:21:48.369415829Z",
     "start_time": "2024-03-11T13:21:45.308808305Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,2), # Use 1-grams and 2-grams.\n",
    "    min_df=0.001,      # Ignore terms that appear in less than 0.1% of the documents.\n",
    "    max_df=0.75,       # Ignore terms that appear in more than 75% of documents.\n",
    "    max_features=1000,  # Use only the top 1000 most frequent words.\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Done: Apply the vectorizer to the clean_text column of each data split \n",
    "# by converting the Dataset object to pandas and using .fit_transform\n",
    "# Remember a Dataset object can be converted to Pandas at any\n",
    "# time by calling .to_pandas(). The output of the vectorizer is a sparse matrix\n",
    "# in Compressed Sparse Row format (CSR), so you will need to apply the .toarray()\n",
    "# method to convert it to a regular NumPy array before building the DataFrame.\n",
    "\n",
    "# Train split\n",
    "train_text_vectors = vectorizer.fit_transform(data[\"train\"].to_pandas()[\"clean_text\"])\n",
    "train_text_vectors = pd.DataFrame(\n",
    "    train_text_vectors.toarray(),\n",
    "    columns=[\"w_\" + w for w in vectorizer.get_feature_names_out()]\n",
    ")\n",
    "\n",
    "# Validation split\n",
    "val_text_vectors = vectorizer.transform(data[\"validation\"].to_pandas()[\"clean_text\"])\n",
    "val_text_vectors = pd.DataFrame(\n",
    "    val_text_vectors.toarray(),\n",
    "    columns=[\"w_\" + w for w in vectorizer.get_feature_names_out()]\n",
    ")\n",
    "\n",
    "# Test split\n",
    "test_text_vectors = vectorizer.transform(data[\"test\"].to_pandas()[\"clean_text\"])\n",
    "test_text_vectors = pd.DataFrame(\n",
    "    test_text_vectors.toarray(),\n",
    "    columns=[\"w_\" + w for w in vectorizer.get_feature_names_out()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical -> One-Hot Conversion\n",
    "\n",
    "Many of the available features in the datasets are categorical, and you will need to convert them to [one-hot vectors](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) in order to use them in a regression model. Luckily, the [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) class is readily available in Scikit-learn for this purpose. An even more convenient approach is to use the [`pandas.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) function, which returns a pandas Dataframe with labeled one-hot encoded columns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:37:22.961046757Z",
     "start_time": "2024-03-11T13:37:22.831740919Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# DONE: Encode the fields `country`, `province` and `variety`\n",
    "# of each data split separately into one-hot vectors \n",
    "# using pd.get_dummies().\n",
    "# Train split\n",
    "train_country_vectors = pd.get_dummies(data[\"train\"].to_pandas()[\"country\"], prefix=\"country\")\n",
    "train_province_vectors = pd.get_dummies(data[\"train\"].to_pandas()[\"province\"], prefix=\"province\")\n",
    "train_variety_vectors = pd.get_dummies(data[\"train\"].to_pandas()[\"variety\"], prefix=\"variety\")\n",
    "\n",
    "# Validation split\n",
    "val_country_vectors = pd.get_dummies(data[\"validation\"].to_pandas()[\"country\"], prefix=\"country\")\n",
    "val_province_vectors = pd.get_dummies(data[\"validation\"].to_pandas()[\"province\"], prefix=\"province\")\n",
    "val_variety_vectors = pd.get_dummies(data[\"validation\"].to_pandas()[\"variety\"], prefix=\"variety\")\n",
    "\n",
    "# Test split\n",
    "test_country_vectors = pd.get_dummies(data[\"test\"].to_pandas()[\"country\"], prefix=\"country\")\n",
    "test_province_vectors = pd.get_dummies(data[\"test\"].to_pandas()[\"province\"], prefix=\"province\")\n",
    "test_variety_vectors = pd.get_dummies(data[\"test\"].to_pandas()[\"variety\"], prefix=\"variety\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all Together and Fitting a Model\n",
    "\n",
    "Now that all the data is ready to be processed, create two Pandas dataframes to train the model: `features` should be the concatenation of the `price` field plus all the vectorized features (`text_vectors`, `country_vectors`, `province_vectors`, `variety_vectors`), while `target` should be a single column of `points` field.\n",
    "\n",
    "Finally, you will train a simple linear model using the `fit` method of an instance of the [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model, which fits a regular least-squares linear regression to the features. A regression model is simply a function that takes a set of numeric values, called **features**, as input, and returns an output score. Fitting a model is the process of finding the right parameters, called **weights**, to map the input features to the output targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T13:44:11.893283994Z",
     "start_time": "2024-03-11T13:44:07.305894581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(n_jobs=-1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Done: Create the features and target dataframes for the train split\n",
    "\n",
    "train_features = pd.concat([\n",
    "    data[\"train\"].to_pandas()[\"price\"],\n",
    "    train_text_vectors,\n",
    "    train_country_vectors,\n",
    "    train_province_vectors,\n",
    "    train_variety_vectors\n",
    "], axis=1)\n",
    "\n",
    "train_target = data[\"train\"].to_pandas()[\"points\"]\n",
    "\n",
    "\n",
    "regressor = LinearRegression(n_jobs=-1)\n",
    "regressor.fit(train_features, train_target)\n",
    "print(regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Baseline\n",
    "\n",
    "Before evaluating the performance of your fitted model, you might want to establish a reasonable **baseline**, representing a null-hypothesis choice. In the case of regression, usually a simple statistical baseline is the mean of the targets, minimizing the prediction error in absence of any information but the distribution of target values. The Scikit-learn library implements a [`DummyRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) that can be used to fit various regression baselines, including the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T14:13:16.166481013Z",
     "start_time": "2024-03-11T14:13:16.123136305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyRegressor()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "baseline = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "baseline.fit(train_features, train_target)\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Having a model is great, but how well does it do? Can it predict what it has seen? We need a way to estimate how well the model will work on new data. We will use two metrics: the [**mean absolute error**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) (MAE), representing the mean positive prediction error across all tested instances, and the [**mean squared error**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) (MSE), where the prediction error is made positive by squaring its value rather than applying the $abs$ operator. The second gives a more intuitive sense of the model's performance, while the first is more robust to outliers, which are upweighted by the squaring operation.\n",
    "\n",
    "Classifying new (held-out) data is called prediction. We reuse the weights we have learned before on a new data matrix to predict the new outcomes. \n",
    "\n",
    "**Important**: the new data needs to have the same number of features! This means using the same vectorizers you fitted on the training split, using only the `.transform` method on the test split.\n",
    "\n",
    "If you didn't apply the vectorization procedure described above to all the splits in `data`, do it now so as to obtain a `features` and `target` dataframe for each split. In the following, we will use `test_features` and `test_target` to evaluate the model using the `.predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T14:16:39.531160943Z",
     "start_time": "2024-03-11T14:16:39.485894421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regressor mean_absolute_error 1.362241590690613\n",
      "Mean baseline mean_absolute_error 2.5211239206335687\n",
      "Linear regressor mean_squared_error 2.9903767897265965\n",
      "Mean baseline mean_squared_error 9.481091498674584\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Done: Repeat the same steps used above for the test split\n",
    "# ...\n",
    "test_features = pd.concat([\n",
    "    data[\"test\"].to_pandas()[\"price\"],\n",
    "    test_text_vectors,\n",
    "    test_country_vectors,\n",
    "    test_province_vectors,\n",
    "    test_variety_vectors\n",
    "], axis=1)\n",
    "test_target = data[\"test\"].to_pandas()[\"points\"]\n",
    "\n",
    "# Done: Use the regressor and the baseline to predict the test target\n",
    "# using the predict method.\n",
    "regressor_predictions = regressor.predict(test_features)\n",
    "baseline_predictions = baseline.predict(test_features)\n",
    "\n",
    "# Print the scores\n",
    "for metric in [mean_absolute_error, mean_squared_error]:\n",
    "    print(\"Linear regressor\", metric.__name__, metric(test_target, regressor_predictions))\n",
    "    print(\"Mean baseline\", metric.__name__, metric(test_target, baseline_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better features = Better model\n",
    "\n",
    "We now have a lot of features! Some are simply tf-idf scores for words that will be totally unrelated to predicting the wine quality, so we might want to discard most of them. Let's select the top 500 based on how well they predict the outcome of the training data.\n",
    "\n",
    "For this purpose, you will use two classes from sklearn, [`SelectKBest`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) (the selection algorithm) and [`chi2`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html) (the selection criterion). Using them in combination will allow you to remove features that are most likely to be independent of the target, and thus not useful for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T14:27:14.285091899Z",
     "start_time": "2024-03-11T14:27:12.207932286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70458, 500)\n",
      "Filtered test features shape: (5000, 500)\n",
      "Filtered linear regressor mean_absolute_error 1.4936798867732295\n",
      "Original linear regressor mean_absolute_error 1.362241590690613\n",
      "Filtered linear regressor mean_squared_error 3.5898227992949234\n",
      "Original linear regressor mean_squared_error 2.9903767897265965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "selector = SelectKBest(chi2, k=500).fit(train_features, train_target)\n",
    "train_filtered_features = selector.transform(train_features)\n",
    "print(train_filtered_features.shape)\n",
    "\n",
    "# Done: Fit another linear regression model to\n",
    "# the filtered features, and compare the performance with the\n",
    "# previous system using the full set of features.\n",
    "test_filtered_features = selector.transform(test_features)\n",
    "print(\"Filtered test features shape:\", test_filtered_features.shape)\n",
    "\n",
    "filtered_regressor = LinearRegression(n_jobs=-1)\n",
    "filtered_regressor.fit(train_filtered_features, train_target)\n",
    "\n",
    "filtered_regressor_predictions = filtered_regressor.predict(test_filtered_features)\n",
    "\n",
    "for metric in [mean_absolute_error, mean_squared_error]:\n",
    "    print(\"Filtered linear regressor\", metric.__name__, metric(test_target, filtered_regressor_predictions))\n",
    "    print(\"Original linear regressor\", metric.__name__, metric(test_target, regressor_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Insights about Salient Features\n",
    "\n",
    "In this exercise we used a simple linear regression model to predict the `points` of a wine given its `description` and its other features. A strength of linear models is that they're highly interpretable: the coefficient assigned to each feature expresses the importance of the said feature in determining the predicted target. For example, a vectorized word having a large positive coefficient given by the trained regression model entails a large predicted quality score for the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T14:35:14.162405360Z",
     "start_time": "2024-03-11T14:35:14.118544095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Feature  Coefficient\n",
      "185      w_gorgeous     6.525690\n",
      "29      w_beautiful     5.804640\n",
      "221          w_lack    -5.597918\n",
      "358        w_simple    -5.529568\n",
      "92        w_complex     5.440893\n",
      "..              ...          ...\n",
      "341          w_rosé    -0.026563\n",
      "446      country_US     0.019834\n",
      "0             price     0.013264\n",
      "75   w_cherry berry    -0.008486\n",
      "441  country_France    -0.000250\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the top 500 features\n",
    "top_scores = selector.scores_.argsort()[-500:]\n",
    "labels = [train_features.columns[i] for i in sorted(top_scores)]\n",
    "\n",
    "# Done: Build and print a dataframe containing the top 500 features\n",
    "# and their respective coefficients, sorted from highest to lowest\n",
    "# Coefficients can be accessed as regressor.coef_[0]\n",
    "top_features_df = pd.DataFrame({'Feature': labels, 'Coefficient': filtered_regressor.coef_})\n",
    "top_features_df = top_features_df.reindex(top_features_df['Coefficient'].abs().sort_values(ascending=False).index)\n",
    "\n",
    "print(top_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude the exercise, comment on the results from the previous operation and the usefulness of textual features in predicting the `points` of a wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Exercise 2: Better Text Features for Wine Scoring\n",
    "\n",
    "In this exercise, you will repeat the same procedure of the previous exercise, but you will use a pre-trained transformer model via the Huggingface Transformers library instead of the TfidfVectorizer.\n",
    "\n",
    "Remember that extracting embeddings from pretrained models is easily done with the `pipeline(\"feature-extraction\")` class, but this can be a very time consuming process even using GPU accelerators. For this reason, consider using small models (e.g. Distilbert) and possibly select a subset of the training instances to make the process faster.\n",
    "\n",
    "**Important**: Since you are now using a model trained on naturally-occurring text, any transformation applied to the text should be ignored and the original text should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-11T13:14:16.521830995Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Reproduce the same pipeline presented above using\n",
    "# features extracted from a transformer model of your choice.\n",
    "# You will still use a Linear Regressor, only the feature from\n",
    "# step 1 will change.\n",
    "\n",
    "# TODO: Compare the performance of the new model with the original\n",
    "# models and baselines. Comment whether it is still possible to\n",
    "# understand the importance of different terms in this new setting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fd5de41c56f1363b4f2c92961d1aea4e026efb5134e6e5487c6cae66bd39229"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
