{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gsarti/ik-nlp-tutorials/blob/main/notebooks/W3E_BPE_Transduction.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T13:03:56.685767557Z",
     "start_time": "2024-02-25T13:03:56.668580293Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run in Colab to install local packages\n",
    "# !pip install spacy transformers sentencepiece tokenizers datasets simplet5\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a BPE tokenizer and a Lexicon-based Transduction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This exercise follows the explanation of using BPE tokenization as explained on Huggingface [Build a Tokenizer from Scratch](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#build-a-tokenizer-from-scratch). Adapted from a notebook by Wietse de Vries*\n",
    "\n",
    "The [Tokenizers](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html) library by Huggingface provides implementations of todayâ€™s most used tokenizers (especially subword-based ones) that is both easy to use and blazing fast (Rust-compiled code!).\n",
    "\n",
    "You will start by exploring the impact of different vocabulary sizes on a subword tokenizer using the Tokenizers library, and how these can be imported and used with spaCy. Finally, you will be asked to train a small transformer model to perform transduction from feminine to masculine words.\n",
    "\n",
    "Exercise 1 is mandatory and will be part of your graded midterm portfolio. Exercise 2 is optional, but we highly recommend you to complete it, especially if you're interested in the \"Modern Neural Networks meet Linguistic Theory\" final project.\n",
    "\n",
    "## Exercise 1: Byte Pair Encoding with Huggingface Tokenizers\n",
    "\n",
    "In the following exercise, we will use a byte-pair encoding (BPE) tokenizer (see Jurafsky & Martin Sec. 2.4.3 and [Sennich et al, 2015](https://aclanthology.org/P16-1162/) to create a vocabulary of frequent words and subwords, allowing us to handle less frequent words.\n",
    "\n",
    "### Setup\n",
    "\n",
    "The following code loads a BPE tokenizer and trainer, tells the system to use whitespace as a separator and defines `[UNK]` as a special token intended to handle unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T12:33:43.709699534Z",
     "start_time": "2024-03-11T12:33:43.707128676Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\"], vocab_size=20000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus\n",
    "\n",
    "The tokenizer creates a dictionary by concatenating characters and substrings into longer strings (possibly full words) based on frequency. So we need a corpus to learn what the most frequent words and substrings are. \n",
    "\n",
    "[Wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) is a dump of the (English) Wikipedia. You can use the `train_from_iterator` method to train from the data in memory, which can be done using the `wikitext` corpus in the Huggingface Datasets library. Alternatively, you can download using wget, or directly from the webpage:\n",
    "\n",
    "```shell\n",
    "wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
    "unzip wikitext-103-raw-v1.zip\n",
    "```\n",
    "\n",
    "The unzipped data is 500 MB. Note that the file extension for the data-files is .raw but the data is just a (unicode) text file. Because this confuses (Ubuntu) Linux, files were renamed to .raw.txt. If you maintain the original .raw filenames, adapt the path below accordingly.\n",
    "\n",
    "### Run the trainer\n",
    "\n",
    "The command below trains the tokenizer on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T13:46:44.839218116Z",
     "start_time": "2024-02-25T13:46:27.758208406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# UNCOMMENT AND ADAPT PATH TO TRAIN ON MANUALLY DOWNLOADED DATASETS\n",
    "# data = [f'wikitext-103-raw/wiki.{split}.raw.txt' for split in ['train','test','valid']]\n",
    "# tokenizer.train(trainer, data)\n",
    "\n",
    "import datasets\n",
    "dataset = datasets.load_dataset(\n",
    "    \"wikitext\", \"wikitext-103-raw-v1\", split=\"train+test+validation\"\n",
    ")\n",
    "\n",
    "# Build a generator to iterate over the dataset\n",
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]\n",
    "\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the tokenizer\n",
    "\n",
    "Now that we have created a vocabulary, we can use it to tokenize a string into words and subtokens (for infrequent words).\n",
    "\n",
    "The example shows that most of the words are included in the vocabulary created by training on Wikipedia text, but that the acronym *UG*, the name *Hanze*, and the word *Applied*, *jointly* and *initiating* are segmented into subword strings. This suggests that these words were not seen during training, or very infrequently. (*UG* occurs 5 times in the training data and *Applied* over 200 times,  also note that the encoding is case-sensitive.). \n",
    "\n",
    "Try a few other examples to get a feeling for the lexical coverage of the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T13:04:12.330060846Z",
     "start_time": "2024-02-25T13:04:12.325902375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'U', 'G', 'and', 'the', 'Han', 'ze', 'University', 'of', 'Ap', 'pl', 'ied', 'Sciences', 'are', 'joint', 'ly', 'initi', 'ating', 'a', 'pilot', 'rapid', 'testing', 'centre', ',', 'which', 'will', 'start', 'on', '18', 'January', '.']\n",
      "25 words and 31 segments\n"
     ]
    }
   ],
   "source": [
    "def show_tokens(text):\n",
    "    output = tokenizer.encode(text)\n",
    "    print(f\"Tokens: {output.tokens}\")\n",
    "    number_of_words = len(tokenizer.pre_tokenizer.pre_tokenize_str(text))\n",
    "    number_of_segments = len(output.tokens)\n",
    "    print(f\"{number_of_words} words and {number_of_segments} segments\")\n",
    "\n",
    "example = \"The UG and the Hanze University of Applied Sciences are jointly initiating a pilot rapid testing centre, which will start on 18 January.\"\n",
    "show_tokens(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Experiment with Vocabulary Size\n",
    "\n",
    "The training data contains 103 M tokens and has a vocabulary size of 267,000 unique types. The default setting for the trainer is to create a dictionary of max 30,000 words. This means that a fair amount of compression takes place. Even more compression can be achieved by setting the vocab_size to a smaller value. \n",
    "\n",
    "1. Choose an example text consisting of at least 100 words. You may want to ensure that it contains some rare words or tokens. \n",
    "\n",
    "2. Experiment with various settings for vocab_size.\n",
    "\n",
    "3. Count the number of words in the example, and the number of segments created by the BPE-tokenizer. Note that if the number segments goes up, more words are segmented into subwords. \n",
    "\n",
    "4. What is the vocabulary size where the number of segments is approx. 150% of the number of words? \n",
    "\n",
    "5. For this setting, what was the longest word in your example text that was not segmented? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T13:05:26.111857323Z",
     "start_time": "2024-02-25T13:04:12.329166564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocab size: 100000, 188 words and 201 segments\n",
      "\n",
      "Vocab size: 30000, 188 words and 219 segments\n",
      "\n",
      "Vocab size: 20000, 188 words and 227 segments\n",
      "\n",
      "Vocab size: 10000, 188 words and 268 segments\n",
      "\n",
      "Vocab size: 8000, 188 words and 297 segments\n",
      "The vocab size where the number of segments is approx. 150% of the number of words is 8000\n",
      "\n",
      "Vocab size: 6500, 188 words and 330 segments\n",
      "\n",
      "\n",
      "Vocab size: 5000, 188 words and 822 segments\n",
      "\n",
      "\n",
      "Vocab size: 1000, 188 words and 822 segments\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test text source: https://en.wikipedia.org/wiki/Saab_JAS_39_Gripen  (154 words according to me, the tokenizer says 188 words)\n",
    "test_text = \"The Saab JAS 39 Gripen is a light single-engine supersonic multirole fighter aircraft manufactured by the Swedish aerospace and defence company Saab AB. The Gripen has a delta wing and canard configuration with relaxed stability design and fly-by-wire flight controls. Later aircraft are fully NATO interoperable. As of 2020, more than 271 Gripens of all models, Aâ€“F, have been delivered. In 1979, the Swedish government began development studies for \\\"an aircraft for fighter, attack, and reconnaissance\\\" (ett jakt-, attack- och spaningsflygplan, hence \\\"JAS\\\") to replace the Saab 35 Draken and 37 Viggen in the Swedish Air Force. A new design from Saab was selected and developed as the JAS 39. The first flight took place in 1988, with delivery of the first serial production airplane in 1993. It entered service with the Swedish Air Force in 1996. Upgraded variants, featuring more advanced avionics and adaptations for longer mission times, began entering service in 2003.\"\n",
    "\n",
    "\n",
    "# loop over different vocab sizes and compare the number of segments with the number of words\n",
    "vocab_sizes = [1000, 5000, 6500, 8000, 10000, 20000, 30000, 100000]\n",
    "vocab_sizes.reverse()\n",
    "for vocab_size in vocab_sizes:\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = BpeTrainer(special_tokens=[\"[UNK]\"],vocab_size=vocab_size)\n",
    "    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))\n",
    "    output = tokenizer.encode(test_text)\n",
    "    number_of_words = len(tokenizer.pre_tokenizer.pre_tokenize_str(test_text))\n",
    "    number_of_segments = len(output.tokens)\n",
    "    print(f\"Vocab size: {vocab_size}, {number_of_words} words and {number_of_segments} segments\")\n",
    "    \n",
    "    # find out when vocab size is 150% of the number of segments\n",
    "    if number_of_segments > 1.5 * number_of_words and number_of_segments < (1.5 * number_of_words * 1.1):\n",
    "        print(f\"The vocab size where the number of segments is approx. 150% of the number of words is {vocab_size}\")\n",
    "    \n",
    "        # find the longest word that was not segmented (question 5)\n",
    "        words = tokenizer.pre_tokenizer.pre_tokenize_str(test_text)\n",
    "        for word in words:\n",
    "            if len(word) == 1:\n",
    "                print(f\"q5: Longest word that was not segmented: {word} with vocab size {vocab_size}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the BPE Tokenizer into spaCy\n",
    "\n",
    "Now that you experimented with the creation of many tokenizers using Huggingface Tokenizers, you might want to move them to a more familiar environment. The following class lets you load a Huggingface Tokenizer into spaCy: the `get_words_spaces` function is used to preserve the whitespaces before tokens that are not word pieces.\n",
    "\n",
    "### Your Turn: Fill in the missing code\n",
    "\n",
    "Your task is to complete the `__call__` method of the `BPETokenizer` class to go from text to spaCy `Docs`, and finally to print the tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# for same reason after running above cell the tokenizer is not working anymore, so I have to redo the setup\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\"], vocab_size=20000) \n",
    "\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"wikitext\", \"wikitext-103-raw-v1\", split=\"train+test+validation\"\n",
    ")\n",
    "\n",
    "# Build a generator to iterate over the dataset\n",
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]\n",
    "\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T13:05:40.687863882Z",
     "start_time": "2024-02-25T13:05:26.152086416Z"
    }
   },
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T13:03:25.676327776Z",
     "start_time": "2024-02-25T13:03:25.566628968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jeff', 'Be', 'z', 'os', 'is', 'a', 'billion', 'aire', 'who', 'became', 'famous', 'after', 'the', 'Dutch', 'bridge', 'controversy', '.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "import spacy\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, tokenizer, vocab):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def get_words_spaces(self, tokens):\n",
    "        words = []\n",
    "        spaces = []\n",
    "        for i, (text, (_, end)) in enumerate(\n",
    "            zip(tokens.tokens, tokens.offsets)\n",
    "        ):\n",
    "            words.append(text)\n",
    "            if i < len(tokens.tokens) - 1:\n",
    "                # If next start != current end we assume a \n",
    "                # space in between\n",
    "                next_start, _ = tokens.offsets[i + 1]\n",
    "                spaces.append(next_start > end)\n",
    "            else:\n",
    "                spaces.append(True)\n",
    "        return words, spaces\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # Encode the texts to obtain tokens\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        # Use get_words_spaces to obtain the words and spaces\n",
    "        words, spaces = self.get_words_spaces(tokens)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.vocab = Vocab(strings=[\n",
    "    tok for tok in tokenizer.get_vocab().keys()\n",
    "])\n",
    "nlp.tokenizer = BPETokenizer(tokenizer, nlp.vocab)\n",
    "\n",
    "text = \"Jeff Bezos is a billionaire who became famous after the Dutch bridge controversy.\"\n",
    "# Convert the text in a list of tokens and print them\n",
    "doc = nlp(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Exercise 2: Lexicon-based Transduction System\n",
    "\n",
    "In this exercise you will build a rule-based tool to transduce a given input text **from masculine to feminine**. You are provided with a list of pairs including feminine words and their masculine counterparts. To create a rule based transducer, the following components will be needed:\n",
    "\n",
    "1. Extract a subset of sentences from the `wikitext-103-raw-v1` containing masculine words (words from the list, gendered pronouns (e.g. he/his/him)). **Tip**: you can try to use the spaCy lemmas annotations to avoid removing inflected forms of words.\n",
    "\n",
    "Fill the `is_masculine` function so that only sentences containing masculine words are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T13:47:07.625064995Z",
     "start_time": "2024-02-25T13:47:01.703282668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text'],\n    num_rows: 207019\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import datasets\n",
    "\n",
    "gender_lexicon = [\n",
    "    (\"Brother\", \"Sister\"),\n",
    "    (\"Drake\", \"Duck\"),\n",
    "    (\"Father\", \"Mother\"),\n",
    "    (\"Gentleman\", \"Lady\"),\n",
    "    (\"Husband\", \"Wife\"),\n",
    "    (\"Man\", \"Woman\"),\n",
    "    (\"Nephew\", \"Niece\"),\n",
    "    (\"Son\", \"Daughter\"),\n",
    "    (\"Wizard\", \"Witch\"),\n",
    "    (\"Boy\", \"Girl\"),\n",
    "    (\"Bull\", \"Cow\"),\n",
    "    (\"Cock\", \"Hen\"),\n",
    "    (\"Dog\", \"Bitch\"),\n",
    "    (\"Drone\", \"Bee\"),\n",
    "    (\"Gander\", \"Goose\"),\n",
    "    (\"Horse\", \"Mare\"),\n",
    "    (\"King\", \"Queen\"),\n",
    "    (\"Monk\", \"Nun\"),\n",
    "    (\"Sir\", \"Madam\"),\n",
    "    (\"Stag\", \"Hind\"),\n",
    "    (\"Stallion\", \"Mare\"),\n",
    "    (\"Tutor\", \"Governess\"),\n",
    "    (\"Drone\", \"Bee\"),\n",
    "    (\"Brother-in-law\", \"Sister-in-law\"),\n",
    "    (\"Son-in-law\", \"Daughter-in-law\"),\n",
    "    (\"Maternal-uncle\", \"Maternal-aunt\"),\n",
    "    (\"Step-son\", \"Step-daughter\"),\n",
    "    (\"Hostess\", \"Steward\"),\n",
    "    (\"Widow\", \"Widower\"),\n",
    "    (\"author\", \"authoress\"),\n",
    "    (\"count\", \"countess\"),\n",
    "    (\"heir\", \"heiress\"),\n",
    "    (\"manager\", \"manageress\"),\n",
    "    (\"patron\", \"patroness\"),\n",
    "    (\"priest\", \"priestess\"),\n",
    "    (\"baron\", \"baroness\"),\n",
    "    (\"giant\", \"giantess\"),\n",
    "    (\"host\", \"hostess\"),\n",
    "    (\"lion\", \"lioness\"),\n",
    "    (\"mayor\", \"mayoress\"),\n",
    "    (\"poet\", \"poetess\"),\n",
    "    (\"shepherd\", \"shepherdess\"),\n",
    "    (\"actor\", \"actress\"),\n",
    "    (\"conductor\", \"conductress\"),\n",
    "    (\"hunter\", \"huntress\"),\n",
    "    (\"prince\", \"princess\"),\n",
    "    (\"traitor\", \"traitress\"),\n",
    "    (\"master\", \"mistress\"),\n",
    "    (\"benefactor\", \"benefactress\"),\n",
    "    (\"founder\", \"foundress\"),\n",
    "    (\"instructor\", \"instructress\"),\n",
    "    (\"emperor\", \"empress\"),\n",
    "    (\"tiger\", \"tigress\"),\n",
    "    (\"waiter\", \"waitress\"),\n",
    "    (\"murderer\", \"murderess\"),\n",
    "    (\"hero\", \"heroine\"),\n",
    "    (\"fox\", \"vixen\"),\n",
    "    (\"sultan\", \"sultana\"),\n",
    "    (\"grandfather\", \"grandmother\"),\n",
    "    (\"manservant\", \"maidservant\"),\n",
    "    (\"milkman\", \"milkwoman\"),\n",
    "    (\"salesman\", \"saleswoman\"),\n",
    "    (\"great-uncle\", \"great-aunt\"),\n",
    "    (\"landlord\", \"landlady\"),\n",
    "    (\"he\", \"she\"),\n",
    "    (\"him\", \"her\"),\n",
    "    (\"his\", \"her\")\n",
    "]\n",
    "\n",
    "def is_masculine(text):\n",
    "    # (use '|'.join(...) to join them in the regex)\n",
    "    masculine_words = []\n",
    "    \n",
    "    for pair in gender_lexicon:\n",
    "        masculine_words.append(pair[0])\n",
    "    \n",
    "    regex = r'\\b(' + '|'.join(masculine_words) + r')\\b'\n",
    "    return bool(re.search(regex, text, re.IGNORECASE))\n",
    "\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"wikitext\", \"wikitext-103-raw-v1\", split=\"train+test+validation\"\n",
    ")\n",
    "\n",
    "# We consider only the first 200 characters to avoid long paragraphs\n",
    "filtered_dataset = dataset.filter(lambda x: is_masculine(x[\"text\"][:200]))\n",
    "filtered_dataset = filtered_dataset.map(lambda x: {\"text\": x[\"text\"][:200]})\n",
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The music was composed by Hitoshi Sakimoto , who had also worked on the previous Valkyria Chronicles games . When he originally heard about the project , he thought it would be a light tone similar t\n",
      " Perhaps because Abraham Lincoln had not yet been inaugurated as President , Captain Totten received no instructions from his superiors and was forced to withdraw his troops . He agreed to surrender t\n",
      " The military force at Little Rock under Dunnington 's command consisted of four officers : himself , Major John B. Lockman , Captain C.C. Green , and 2nd Lt. W.W. Murphy . In addition to these , he h\n",
      " Lt. Col. Dunnington continued to build up his works at Little Rock until November 1862 , when Captain Sanford C. Faulkner ( composer of The Arkansas Traveler ) was placed in charge of the Arsenal . D\n",
      " Barker was born the second daughter and youngest child of Walter Barker , a partner in a seed supply company and an amateur artist , and his wife Mary Eleanor ( Oswald ) Barker on 28 June 1895 at hom\n"
     ]
    }
   ],
   "source": [
    "# print some examples from the filtered dataset\n",
    "for i in range(5):\n",
    "    print(filtered_dataset[i][\"text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T13:17:55.543140083Z",
     "start_time": "2024-02-25T13:17:55.540695327Z"
    }
   },
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a `feminize` function that takes a sententence from the the filtered dataset and returns a feminized version of it, based on lexical pairs. Use it to create a new field \"feminine_text\" in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'feminine_text'],\n    num_rows: 207019\n})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feminize(text):\n",
    "    \"\"\"Returns a feminized version of text\"\"\"\n",
    "    feminized_text = text\n",
    "    for m, f in gender_lexicon:\n",
    "        match_regex = r'\\b' + re.escape(m) + r'\\b' \n",
    "        feminized_text = re.sub(match_regex, f, feminized_text, flags=re.IGNORECASE)\n",
    "    return feminized_text\n",
    "\n",
    "# TODO: Use filtered_dataset.map to add a feminized version of the text column\n",
    "feminized_dataset = filtered_dataset.map(lambda x: {\"feminine_text\": feminize(x[\"text\"])})\n",
    "feminized_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T13:47:18.030837545Z",
     "start_time": "2024-02-25T13:47:18.025293504Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The music was composed by Hitoshi Sakimoto , who had also worked on the previous Valkyria Chronicles games . When she originally heard about the project , she thought it would be a light tone similar t\n",
      " Perhaps because Abraham Lincoln had not yet been inaugurated as President , Captain Totten received no instructions from her superiors and was forced to withdraw her troops . she agreed to surrender t\n",
      " The military force at Little Rock under Dunnington 's command consisted of four officers : himself , Major John B. Lockman , Captain C.C. Green , and 2nd Lt. W.W. Murphy . In addition to these , she h\n",
      " Lt. Col. Dunnington continued to build up her works at Little Rock until November 1862 , when Captain Sanford C. Faulkner ( composer of The Arkansas Traveler ) was placed in charge of the Arsenal . D\n",
      " Barker was born the second daughter and youngest child of Walter Barker , a partner in a seed supply company and an amateur artist , and her wife Mary Eleanor ( Oswald ) Barker on 28 June 1895 at hom\n"
     ]
    }
   ],
   "source": [
    "# print some examples from the feminized dataset\n",
    "for i in range(5):\n",
    "    print(feminized_dataset[i][\"feminine_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T13:47:26.822854764Z",
     "start_time": "2024-02-25T13:47:26.778560939Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Rename the `text` field to `source_text` and the `feminine_text` field to `target_text` (this is needed for `SimpleT5` to work properly). Transform the dataset to Pandas DataFrame format and use the following code to train a simple neural transduction model.\n",
    "\n",
    "*(More info on the [T5 model](https://huggingface.co/t5-small) and the [SimpleT5](https://github.com/Shivanandroy/simpleT5) library)*"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# clear gpu memory\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# clear all venv gpu memory\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T13:48:10.084068779Z",
     "start_time": "2024-02-25T13:48:10.081104953Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T14:29:14.507124760Z",
     "start_time": "2024-02-25T13:48:12.758287878Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation sanity check: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8902d4f88cfb4cf49d3c35b7bb378ad5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/home/matisse/ik-nlp-tutorials/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "237b546513df4be4957605782f3e69e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb411daae0ed4950b1f1e67841cddf7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e114230ada34f119074033e0456bb84"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90d2fb4179d14ed7ab469d0e93797f16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from simplet5 import SimpleT5\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Convert the Huggingface Dataset in a Pandas dataframe and split it in training\n",
    "# and evaluation sets (you decide the sizes based on your computational resources)\n",
    "df = pd.DataFrame(feminized_dataset)\n",
    "df.rename(columns={\"text\": \"source_text\", \"feminine_text\": \"target_text\"}, inplace=True)\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "\n",
    "model = SimpleT5()\n",
    "model.from_pretrained(model_type=\"t5\", model_name=\"t5-small\")\n",
    "model.train(\n",
    "    train_df=train_df,\n",
    "    eval_df=eval_df, \n",
    "    source_max_token_len=128, \n",
    "    target_max_token_len=128, \n",
    "    batch_size=40, max_epochs=3, use_gpu=torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleT5' object has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# save the model to current directory named \"t5-modeltje\"\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_model\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'SimpleT5' object has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "# save the model to current directory named \"t5-modeltje\"\n",
    "model.save_model(\"model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T14:31:37.054818381Z",
     "start_time": "2024-02-25T14:31:37.011432921Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Conclude by testing the model on a few examples of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T14:32:03.149948606Z",
     "start_time": "2024-02-25T14:32:02.297784250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['my Sister thought that her uncle was a duke']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_model(\"t5\", \"outputs/simplet5-epoch-2-train-loss-0.0009-val-loss-0.0002\", use_gpu=torch.cuda.is_available())\n",
    "\n",
    "text_to_feminize = \"my brother thought that his uncle was a duke\"\n",
    "model.predict(text_to_feminize)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85d98b63f393548f3009c8d52d8286e609a1467b1184fe464fb700873fbd3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
